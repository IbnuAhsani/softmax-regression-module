# -*- coding: utf-8 -*-
"""Multinomial Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EKf8fGhgNolOaCy2QRD1t9sZJh9s4oA3

[**The link for the tutorial**](https://www.youtube.com/watch?v=2JiXktBn_2M)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import numpy as np
import pandas as pd
import seaborn as sn
import tensorflow as tf
from imblearn.under_sampling import NearMiss
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
from matplotlib import pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split, KFold

np.set_printoptions(threshold=sys.maxsize)

# %matplotlib inline

"""**Example Program**"""

# from tensorflow.examples.tutorials.mnist import input_data
# mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# graph = tf.Graph()
# with graph.as_default():

#   batch_size = 128
#   beta = .001
#   image_size = 28
#   num_labels = 10

#   tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))
#   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
#   tf_valid_dataset = tf.constant(mnist.validation.images)
#   tf_test_dataset = tf.constant(mnist.test.images)

#   w_logit = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))
#   b_logit = tf.Variable(tf.zeros([num_labels]))

#   def model(data):
#     return tf.matmul(data, w_logit) + b_logit

#   logits = model(tf_train_dataset)
#   loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))
#   regularized_loss = tf.nn.l2_loss(w_logit) 
#   total_loss = loss + beta + regularized_loss

#   optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(total_loss)

#   train_prediction = tf.nn.softmax(logits)
#   valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
#   test_prediction = tf.nn.softmax(model(tf_test_dataset))

# num_step = 1001

# with tf.Session(graph=graph) as session:
#   tf.global_variables_initializer().run()
#   print("initialized")

#   for step in range(num_step):
#     batch_data, batch_labels = mnist.train.next_batch(batch_size=batch_size)
#     feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}

#     _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)

#     if (step % 500 == 0):
#       print("minibatch loss at step %d: %f" % (step, l))
#       print("minibatch accuracy: %.1f%%" % get_accuracy(batch_labels, predictions))
#       print("validation accuracy: %.1f%%" % get_accuracy(mnist.validation.labels, valid_prediction.eval()))
#       print("\n")
  
#   print("test accuracy: %.1f%%" % get_accuracy(mnist.test.labels, test_prediction.eval()))

"""**Data Preprocessing**"""

data_frame = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Content-Based Recommender System Data/Hypothesis Data/data-17-fv-tf-idf-scores-200.csv', header=None, sep=',')
data_frame.shape

header = []

for i in range(len(data_frame.columns)-1):
  header.append(str(i))

header.append('target')

data_frame.columns = header
data_frame.columns

value_counts = data_frame['target'].value_counts()
value_counts.plot(kind='bar')
value_counts.rename_axis('journal_id').to_frame('counts')

data_labels = data_frame['target']
data_features = data_frame.drop('target',axis=1)

near_miss = SMOTE()
x_undersampled, y_undersampled = near_miss.fit_sample(data_features, data_labels)

print(x_undersampled.shape)
print(y_undersampled.shape)

data_labels = data_frame['target']
data_features = data_frame.drop('target',axis=1)

near_miss = NearMiss(random_state=42)
x_undersampled, y_undersampled = near_miss.fit_sample(data_features, data_labels)

print(x_undersampled.shape)
print(y_undersampled.shape)

# data_labels = data_frame['target']
# data_features = data_frame.drop('target',axis=1)

# smk = SMOTETomek(random_state=42)
# x_undersampled, y_undersampled = smk.fit_sample(data_features, data_labels)

# print(x_undersampled.shape)
# print(y_undersampled.shape)

data_features_df = pd.DataFrame(data = x_undersampled[0:,0:], 
                                index = [i for i in range(x_undersampled.shape[0])],
                                columns = [str(i) for i in range(x_undersampled.shape[1])])

data_labels_df = pd.DataFrame(data = y_undersampled[0:], 
                                index = [i for i in range(y_undersampled.shape[0])],
                                columns = ['target'])

data_frame_undersampled = data_features_df.join(data_labels_df)

print(data_features_df.head())
print(data_labels_df.head())
print(data_frame_undersampled.head())

value_counts_undersampled = data_labels_df['target'].value_counts()
value_counts_undersampled.plot(kind='bar')
value_counts_undersampled.rename_axis('journal_id').to_frame('counts')

data_frame_shuffled_once = data_frame_undersampled.sample(frac=1)
data_frame_shuffled_twice = data_frame_shuffled_once.sample(frac=1)
data_frame_shuffled_twice.head()

"""**Program Train Test Split Data**"""

data_train, data_test = train_test_split(data_frame_shuffled_twice, test_size = 0.2)

data_train_features = data_train.drop('target',axis=1)
data_train_labels = data_train['target']

data_test_features = data_test.drop('target',axis=1)
data_test_labels = data_test['target']

batch_size = 32
beta = .001
learning_rate = 0.001
num_epoch = 5
num_features = data_frame.shape[1] - 1
num_labels = value_counts.shape[0]

def to_onehot(y):
  data = np.zeros((num_labels))
  data[y] = 1
  return data

data_train_labels_one_hot_encoded = np.array([to_onehot(label) for label in data_train_labels])
data_test_labels_one_hot_encoded = np.array([to_onehot(label) for label in data_test_labels])

print(data_train_labels_one_hot_encoded.shape)
print(data_test_labels_one_hot_encoded.shape)

def to_label_list(results):
  label_list = []

  for result in results:
    prediction_label = np.argmax(result)
    label_list.append(prediction_label)
  
  return label_list

def get_accuracy(labels, predictions):
  test_batch_size = predictions.shape[0]
  total_correct_prediction = np.sum(np.argmax(predictions, axis=1) == np.argmax(labels, axis=1))
  accuracy = 100.0 * total_correct_prediction / test_batch_size  
  
  return accuracy

def get_fmeasure(labels, predictions):
  
  fmeasures = {}

  for i in range(num_labels):
    fmeasure_data_dict = {
      "gi": 0,
      "pi_intersect_gi": 0,
    }

    fmeasures[str(i)] = fmeasure_data_dict 
    fmeasure_data_dict = {}

  for i in range(len(labels)):
    label = labels[i]
    prediction = predictions[i]
    fmeasures[str(label)]['gi'] += 1

    if prediction == label:
      fmeasures[str(label)]['pi_intersect_gi'] += 1
  
  total_fmeasure_score = 0

  for i in range(num_labels):
    fmeasure_data = fmeasures[str(i)]
    
    gi = fmeasure_data['gi']
    pi_intersect_gi = fmeasure_data['pi_intersect_gi']
    
    fmeasure_score = (2 * pi_intersect_gi)/(2 * gi)
    total_fmeasure_score += fmeasure_score

  fmeasure_score = total_fmeasure_score / num_labels

  return fmeasure_score

graph = tf.Graph()

with graph.as_default():

  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_test_dataset = tf.constant(data_test_features, dtype=tf.float32)

  w_logit = tf.Variable(tf.truncated_normal([num_features, num_labels]))
  b_logit = tf.Variable(tf.zeros([num_labels]))

  def model(data):
    return tf.matmul(data, w_logit) + b_logit

  logits = model(tf_train_dataset)
  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))
  regularized_loss = tf.nn.l2_loss(w_logit) 
  total_loss = loss + beta + regularized_loss

  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)

  train_prediction = tf.nn.softmax(logits)
  test_prediction = tf.nn.softmax(model(tf_test_dataset))

with tf.Session(graph=graph) as session:
  tf.global_variables_initializer().run()
  print("initialized")

  avg_batch_loss_list = []
  batch_accuracy_list = []
  total_batch = len(data_train_features)//batch_size

  for epoch in range(num_epoch):
    
    total_loss = 0

    for i in range(0, (total_batch * batch_size), batch_size):
      batch_index = range(i, i+1*batch_size)
      batch_data = data_train_features[i:i+1*batch_size]
      batch_labels = data_train_labels_one_hot_encoded[i:i+1*batch_size]

      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)

      total_loss += l

      if (i == (total_batch * batch_size) - batch_size):
        batch_accuracy = get_accuracy(batch_labels, predictions)
        batch_accuracy_list.append(batch_accuracy)

        avg_batch_loss = total_loss / total_batch
        avg_batch_loss_list.append(avg_batch_loss)
        
        print("epoch: %d" % epoch)
        print("minibatch loss: %f" % avg_batch_loss)
        print("minibatch accuracy: %.1f%%" % batch_accuracy)
        print("\n")

  epochs_range = range(0, num_epoch)

  plt.plot(epochs_range, avg_batch_loss_list, 'g', label='Training Loss')
  plt.title('Training Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

  plt.plot(epochs_range, batch_accuracy_list, 'b', label='Batch Accuracy')
  plt.title('Batch Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.show()

  test_predictions = test_prediction.eval()
  test_accuracy = get_accuracy(test_predictions, data_test_labels_one_hot_encoded)
  
  print()
  print("test accuracy: %.1f%%" % test_accuracy)

  data_test_label_list = to_label_list(data_test_labels_one_hot_encoded)
  test_predictions_label_list = to_label_list(test_predictions)

  test_result_fmeasure = get_fmeasure(data_test_label_list, test_predictions_label_list)

  print()
  print('test_result_fmeasure')
  print(test_result_fmeasure)
  print()

  test_result_confusion_matrix = confusion_matrix(data_test_label_list, test_predictions_label_list)
  
  print()
  print('test_result_confusion_matrix')
  print(test_result_confusion_matrix)
  print()

  test_classification_report = classification_report(data_test_label_list, test_predictions_label_list)

  print()
  print('test_classification_report')
  print(test_classification_report)
  print()

  confusion_matrix_df = pd.DataFrame(test_result_confusion_matrix, range(9), range(9))
  plt.figure(figsize=(12,9))
  sn.set(font_scale=1.0)
  sn.heatmap(confusion_matrix_df, annot=True, fmt='g', cmap="BuPu", annot_kws={"size": 12}) 

  plt.show()

"""---



---



---



---

**FAILED Program KFOLD**
"""

batch_size = 32
beta = .001
learning_rate = 0.001
num_epoch = 101
num_features = 1561
num_labels = 9
num_k_splits = 10

def to_onehot(y):
  data = np.zeros((num_labels))
  data[y] = 1
  return data

history = {}
fold_counter = 1
kfold = KFold(n_splits= num_k_splits, random_state=None, shuffle=True)

for train_index, test_index in kfold.split(data_frame_shuffled_twice):

  print("\n==================================")
  print("Fold: %d" % fold_counter)

  k_fold_train_data = data_frame_shuffled_twice.loc[train_index, : ]
  k_fold_train_features = k_fold_train_data.drop('target',axis=1).to_numpy()
  k_fold_train_labels = k_fold_train_data['target']
  k_fold_train_labels_one_hot_encoded = np.array([to_onehot(label) for label in k_fold_train_labels])

  k_fold_test_data = data_frame_shuffled_twice.loc[test_index, : ]
  k_fold_test_features = k_fold_test_data.drop('target',axis=1).to_numpy()
  k_fold_test_labels = k_fold_test_data['target']
  k_fold_test_labels_one_hot_encoded = np.array([to_onehot(label) for label in k_fold_test_labels])

  reindexed_train_index = np.arange(len(train_index)) 

  graph = tf.Graph()

  with graph.as_default():

    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
    tf_test_dataset = tf.constant(k_fold_test_features, dtype=tf.float32)

    w_logit = tf.Variable(tf.truncated_normal([num_features, num_labels]))
    b_logit = tf.Variable(tf.zeros([num_labels]))

    def model(data):
      return tf.matmul(data, w_logit) + b_logit

    logits = model(tf_train_dataset)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))
    regularized_loss = tf.nn.l2_loss(w_logit) 
    total_loss = loss + beta + regularized_loss

    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)

    train_prediction = tf.nn.softmax(logits)
    test_prediction = tf.nn.softmax(model(tf_test_dataset))

  with tf.Session(graph=graph) as session:
    tf.global_variables_initializer().run()

    avg_batch_loss_list = []
    batch_accuracy_list = []
    total_batch = len(train_index)//batch_size
    
    for epoch in range(num_epoch):

      total_loss = 0

      for i in range(0, (total_batch * batch_size), batch_size):

        batch_index = reindexed_train_index[i : i+1*batch_size]
        batch_features = k_fold_train_features[batch_index, : ]
        batch_labels = k_fold_train_labels_one_hot_encoded[batch_index, : ]

        feed_dict = {tf_train_dataset : batch_features, tf_train_labels : batch_labels}
        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)

        total_loss += l

      batch_accuracy = get_accuracy(batch_labels, predictions)
      batch_accuracy_list.append(batch_accuracy)

      avg_batch_loss = total_loss / total_batch
      avg_batch_loss_list.append(avg_batch_loss)
      
      print("epoch: %d" % epoch)
      print("minibatch loss: %f" % avg_batch_loss)
      print("minibatch accuracy: %.1f%%" % batch_accuracy)
      print("\n")

    test_accuracy = get_accuracy(k_fold_test_labels_one_hot_encoded, test_prediction.eval())
    
    print("test accuracy: %.1f%%" % test_accuracy)
    print("\n")

  tf.reset_default_graph()

  history_dict = {
      "batch_loss": avg_batch_loss_list,
      "batch_accuracy": batch_accuracy_list,
      "test_accuracy": test_accuracy 
  }

  history[str(fold_counter)] = history_dict
  history_dict = {}
  fold_counter += 1

for epoch in history:
  print("\n==================================")
  print("Epoch: %s" % epoch)

  history_batch_loss = history[epoch]['batch_loss']
  history_batch_accuracy = history[epoch]['batch_accuracy']
  history_test_accuracy = history[epoch]['test_accuracy']

  epochs_range = range(0, num_epoch)

  plt.plot(epochs_range, history_batch_loss, 'g', label='Training Loss')
  plt.title('Training Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

  plt.plot(epochs_range, history_batch_accuracy, 'b', label='Batch Accuracy')
  plt.title('Batch Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.show()

  print("Test Accuracy: %.1f%%" % history_test_accuracy)
  print("\n")