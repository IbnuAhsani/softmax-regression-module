# -*- coding: utf-8 -*-
"""Multinomial Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EKf8fGhgNolOaCy2QRD1t9sZJh9s4oA3

[**The link for the tutorial**](https://www.youtube.com/watch?v=2JiXktBn_2M)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

# Commented out IPython magic to ensure Python compatibility.
import os
import sys
import numpy as np
import pandas as pd
import tensorflow as tf
from imblearn.under_sampling import NearMiss
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split

np.set_printoptions(threshold=sys.maxsize)

# %matplotlib inline

def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])

"""**Example Program**"""

# from tensorflow.examples.tutorials.mnist import input_data
# mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

# graph = tf.Graph()
# with graph.as_default():

#   batch_size = 128
#   beta = .001
#   image_size = 28
#   num_labels = 10

#   tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))
#   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
#   tf_valid_dataset = tf.constant(mnist.validation.images)
#   tf_test_dataset = tf.constant(mnist.test.images)

#   w_logit = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels]))
#   b_logit = tf.Variable(tf.zeros([num_labels]))

#   def model(data):
#     return tf.matmul(data, w_logit) + b_logit

#   logits = model(tf_train_dataset)
#   loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))
#   regularized_loss = tf.nn.l2_loss(w_logit) 
#   total_loss = loss + beta + regularized_loss

#   optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(total_loss)

#   train_prediction = tf.nn.softmax(logits)
#   valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
#   test_prediction = tf.nn.softmax(model(tf_test_dataset))

# num_step = 1001

# with tf.Session(graph=graph) as session:
#   tf.global_variables_initializer().run()
#   print("initialized")

#   for step in range(num_step):
#     batch_data, batch_labels = mnist.train.next_batch(batch_size=batch_size)
#     feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}

#     _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)

#     if (step % 500 == 0):
#       print("minibatch loss at step %d: %f" % (step, l))
#       print("minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
#       print("validation accuracy: %.1f%%" % accuracy(valid_prediction.eval(), mnist.validation.labels))
#       print("\n")
  
#   print("test accuracy: %.1f%%" % accuracy(test_prediction.eval(), mnist.test.labels))

"""**Actual Program**"""

data_frame = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Content-Based Recommender System Data/Hypothesis Data/data-16-fv-tf-idf-scores-7561-row-9-class-200-feature.csv', header=None, sep=',')
data_frame.shape

header = []

for i in range(len(data_frame.columns)-1):
  header.append(str(i))

header.append('target')

data_frame.columns = header
data_frame.columns

value_counts = data_frame['target'].value_counts()
value_counts.plot(kind='bar')
value_counts.rename_axis('journal_id').to_frame('counts')

data_labels = data_frame['target']
data_features = data_frame.drop('target',axis=1)

near_miss = NearMiss(random_state=42)
x_undersampled, y_undersampled = near_miss.fit_sample(data_features, data_labels)

print(x_undersampled.shape)
print(y_undersampled.shape)

data_features_df = pd.DataFrame(data = x_undersampled[0:,0:], 
                                index = [i for i in range(x_undersampled.shape[0])],
                                columns = [str(i) for i in range(x_undersampled.shape[1])])

data_labels_df = pd.DataFrame(data = y_undersampled[0:], 
                                index = [i for i in range(y_undersampled.shape[0])],
                                columns = ['target'])

data_frame_undersampled = data_features_df.join(data_labels_df)

print(data_features_df.head())
print(data_labels_df.head())
print(data_frame_undersampled.head())

value_counts_undersampled = data_labels_df['target'].value_counts()
value_counts_undersampled.plot(kind='bar')
value_counts_undersampled.rename_axis('journal_id').to_frame('counts')

data_frame_shuffled_once = data_frame_undersampled.sample(frac=1)
data_frame_shuffled_four = data_frame_shuffled_once.sample(frac=1)
data_frame_shuffled_twice = data_frame_shuffled_four.sample(frac=1)
data_frame_shuffled_twice.head()

data_train, data_test = train_test_split(data_frame_shuffled_twice, test_size = 0.2)

data_train_features = data_train.drop('target',axis=1)
data_train_labels = data_train['target']

data_test_features = data_test.drop('target',axis=1)
data_test_labels = data_test['target']

batch_size = 128
beta = .001
learning_rate = 0.001
num_epoch = 101
num_features = 1565
num_labels = 9

def to_onehot(y):
  data = np.zeros((num_labels))
  data[y] = 1
  return data

data_train_labels_one_hot_encoded = np.array([to_onehot(label) for label in data_train_labels])
data_test_labels_one_hot_encoded = np.array([to_onehot(label) for label in data_test_labels])

print(data_train_labels_one_hot_encoded.shape)
print(data_test_labels_one_hot_encoded.shape)

graph = tf.Graph()

with graph.as_default():

  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, num_features))
  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))
  tf_test_dataset = tf.constant(data_test_features, dtype=tf.float32)

  w_logit = tf.Variable(tf.truncated_normal([num_features, num_labels]))
  b_logit = tf.Variable(tf.zeros([num_labels]))

  def model(data):
    return tf.matmul(data, w_logit) + b_logit

  logits = model(tf_train_dataset)
  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf_train_labels))
  regularized_loss = tf.nn.l2_loss(w_logit) 
  total_loss = loss + beta + regularized_loss

  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)

  train_prediction = tf.nn.softmax(logits)
  test_prediction = tf.nn.softmax(model(tf_test_dataset))

with tf.Session(graph=graph) as session:
  tf.global_variables_initializer().run()
  print("initialized")

  avg_batch_loss_list = []
  batch_accuracy_list = []
  total_batch = len(data_frame)//batch_size

  for epoch in range(num_epoch):

    total_loss = 0

    for i in range(total_batch):

      batch_data = data_train_features[i:i+1*batch_size]
      batch_labels = data_train_labels_one_hot_encoded[i:i+1*batch_size]

      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
      _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)

      total_loss += l

      if (i == total_batch - 1):
        batch_accuracy = accuracy(predictions, batch_labels)
        batch_accuracy_list.append(batch_accuracy)

        avg_batch_loss = total_loss / total_batch
        avg_batch_loss_list.append(avg_batch_loss)
        
        print("epoch: %d" % epoch)
        print("minibatch loss: %f" % avg_batch_loss)
        print("minibatch accuracy: %.1f%%" % batch_accuracy)
        print("\n")

  epochs_range = range(0, num_epoch)

  plt.plot(epochs_range, avg_batch_loss_list, 'g', label='Training Loss')
  plt.title('Training Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.show()

  plt.plot(epochs_range, batch_accuracy_list, 'b', label='Batch Accuracy')
  plt.title('Batch Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend()
  plt.show()

  print("test accuracy: %.1f%%" % accuracy(test_prediction.eval(), data_test_labels_one_hot_encoded))