# -*- coding: utf-8 -*-
"""Softmax Regression Module - Content-Based Recommender System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ucad5WqSBmplsjTSBFM2N76mndIwSe1R

**Import Libs**
"""

import tensorflow as tf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""**Load Dataset**"""

data_frame = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Content-Based Recommender System Data/fv-tf-idf-scores.csv', header=None, sep=',')

header = []

for i in range(len(data_frame.columns)-1):
  header.append(str(i))

header.append('target')
data_frame.columns = header

data_frame.head()

x = data_frame.drop('target',axis=1)
y = data_frame['target']

x_train_df, x_test_df, y_train_df, y_test_df = train_test_split(x, y, test_size=0.2)

x_train = x_train_df.to_numpy()
x_test = x_test_df.to_numpy()
y_train = y_train_df.to_numpy()
y_test = y_test_df.to_numpy()

print(x_train.shape, x_test.shape)
print(y_train.shape, y_test.shape)

"""**Hyperparameters**"""

num_features = 500
num_labels = 5
learning_rate = 0.001
batch_size = 3
training_epochs = 10
display_step = 1

"""**One-Hot Encoding**"""

def to_onehot(y):
  data = np.zeros((num_labels))
  data[y] = 1
  return data

x_train_one_hot_encoded = np.reshape(x_train, (-1, num_features))
x_test_one_hot_encoded = np.reshape(x_test, (-1, num_features))

y_train_one_hot_encoded = np.array([to_onehot(y) for y in y_train])
y_test_one_hot_encoded = np.array([to_onehot(y) for y in y_test])

print(x_train_one_hot_encoded.shape, x_test_one_hot_encoded.shape)
print(y_train_one_hot_encoded.shape, y_test_one_hot_encoded.shape)

"""**Model**"""

x = tf.placeholder(tf.float32, shape = [None, num_features])
y = tf.placeholder(tf.float32, shape = [None, num_labels])

weight = tf.Variable(tf.zeros([num_features, num_labels]))
bias = tf.Variable(tf.zeros([num_labels]))

pred = tf.nn.softmax(tf.matmul(x, weight) + bias)
cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices = 1))
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

init = tf.global_variables_initializer()

def get_accuracy(predictions, labels):
  correctly_predicted = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
  accuracy = (100.0 * correctly_predicted) / predictions.shape[0]
  return accuracy

with tf.Session() as sess:
  # Run the initializer
  sess.run(init)
  
  # Training cycle
  for epoch in range(training_epochs):
    avg_cost = 0.0
    avg_acc = 0.0
    total_batch = len(x_train_one_hot_encoded)//batch_size
    
    for i in range(total_batch):
      batch_x = x_train_one_hot_encoded[i:i+1*batch_size]
      batch_y = y_train_one_hot_encoded[i:i+1*batch_size]
            
      _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})
      avg_cost += c/total_batch
            
      pred_y = sess.run(pred, feed_dict={x: batch_x})
      acc = get_accuracy(pred_y, batch_y)
      avg_acc += acc/total_batch
            
            
      if (epoch+1) % display_step == 0:
        tc = sess.run(cost, feed_dict={x: x_test_one_hot_encoded, y: y_test_one_hot_encoded})
        pred_y = sess.run(pred, feed_dict={x: x_test_one_hot_encoded})
        ta = get_accuracy(pred_y, y_test_one_hot_encoded)
            
        print("Epoch: {:2.0f} - Cost: {:1.5f} - Acc: {:0.5f} - Test Cost: {:0.5f} - Test Acc: {:0.5f}".format(
            epoch+1, avg_cost, avg_acc, tc, ta))
            
  print("Optimization Finshed")

  # Test model
  correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
  # Calculate accuracy
  acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
  print("Test Accuracy:", acc.eval({x: x_test_one_hot_encoded, y: y_test_one_hot_encoded}))

